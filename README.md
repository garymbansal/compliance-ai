# compliance-ai
This Compliance Advisor system is driven with vector embeddings, LLMs, RAG, LangChain, MLOps etc to provide a chatbot capability to users governed by a government agency. 

##Target Architecture

```
compliance-ai/
â”œâ”€ README.md
â”œâ”€ infra/              # optional, handles VPC, KMS, vector DB, etc.
â”œâ”€ services/
â”‚  â”œâ”€ ingestion/       # transforms PDFs â†’ vector embeddings
â”‚  â”œâ”€ policy-service/  # handles user auth, step-up, sensitivity rules
â”‚  â”œâ”€ orchestrator/    # LangChain / RAG orchestration
â”‚  â”œâ”€ model-gateway/   # routes to private vs managed LLM
â”‚  â””â”€ audit-service/   # append-only logging of queries & model responses
â”œâ”€ docs/               # architecture diagrams, playbooks
â”œâ”€ tests/
â”œâ”€ examples/
â””â”€ .github/workflows/

```

```mermaid
flowchart TD
    %% Data Sources
    subgraph Data
        A["S3: MAS PDFs / Circulars"] 
    end

    %% Ingestion Service
    subgraph Ingestion["Ingestion Service"]
        B["s3_stream.py: Secure Stream"] 
        C["Extractor: extract_text_from_pdf_stream"] 
        D["Redactor: redact_pii"] 
        E["Classifier: public/internal/restricted"] 
        F["Chunker: split into embedding chunks"] 
        G["Embedder: text â†’ vectors"] 
        H["Chroma Store: partitioned collections"]
        I["Audit Log: log_ingestion_event"]
    end

    %% Policy Service
    subgraph Policy["Policy Service"]
        J["User Auth and Role Check (SSO)"] 
        K["Sensitivity Rules / Step-up Auth for Restricted Docs"]
    end

    %% Orchestrator / LangChain
    subgraph Orchestrator["LangChain Orchestrator"]
        L["Prompt & Context Manager"] 
        M["RAG Retrieval"] 
        N["Append Document Snippets + Citation IDs"]
    end

    %% Model Gateway
    subgraph ModelGateway["Model Gateway"]
        O["Route to Private LLM (VPC) or Managed LLM"] 
        P["Inference"]
    end

    %% Audit Service
    subgraph Audit["Audit Service"]
        Q["Immutable Logs: query, model version, context IDs"]
    end

    %% Flows
    A --> B
    B --> C --> D --> E --> F --> G --> H --> I

    J --> K

    %% Retrieval paths with sensitivity checks
    H -->|Public/Internal Docs| M
    H -->|Restricted Docs| K --> M

    M --> L --> N --> O --> P --> Q
```
## Data Ingestion
```
services/ingestion/
â”œâ”€ src/
â”‚  â”œâ”€ ingestion/
â”‚  â”‚  â”œâ”€ s3_stream.py
â”‚  â”‚  â”œâ”€ extractor.py
â”‚  â”‚  â”œâ”€ redactor.py
â”‚  â”‚  â”œâ”€ classifier.py
â”‚  â”‚  â”œâ”€ chunker.py
â”‚  â”‚  â”œâ”€ embedder.py
â”‚  â”‚  â”œâ”€ chroma_store.py
â”‚  â”‚  â”œâ”€ audit.py
â”‚  â”‚  â””â”€ pipeline.py
â”‚  â””â”€ config.py
â”œâ”€ ingest_cli.py
â”œâ”€ requirements.txt
â””â”€ Dockerfile
```
`pipeline.py` orchestrates all ingestion steps and serves as the **main entry point** for the system.

Each module follows the **single-responsibility principle** for clarity and maintainability:

## ðŸ”§ Modules

- **`s3_stream.py`** â€“ Secure streaming of documents from Amazon S3.
- **`extractor.py`** â€“ Parses and extracts text from PDF files.
- **`redactor.py`** â€“ Performs PII redaction to ensure privacy compliance.
- **`classifier.py`** â€“ Classifies content based on sensitivity levels.
- **`chunker.py`** â€“ Splits text into embedding-ready chunks.
- **`embedder.py`** â€“ Converts text chunks into vector embeddings.
- **`chroma_store.py`** â€“ Stores embeddings in partitioned Chroma collections.
- **`audit.py`** â€“ Logs ingestion events to DynamoDB or stdout (configurable).
- **`ingest_cli.py`** â€“ CLI entry point for triggering ingestion locally or within containers.

## ðŸš€ Usage

To run the ingestion pipeline locally or in a containerized environment:

```bash
python ingest_cli.py --source s3 --bucket your-bucket-name
